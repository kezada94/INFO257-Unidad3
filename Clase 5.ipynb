{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dca597",
   "metadata": {},
   "source": [
    "## Clase 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b21cc",
   "metadata": {},
   "source": [
    "La Regresion Logistica es un modelo simple utilizado para realizar clasificacion binaria. Su mayor limitante esta en que **no** es capaz de clasificar datos que no son linealmente separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c6c44",
   "metadata": {},
   "source": [
    "![img](xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabeb80",
   "metadata": {},
   "source": [
    "Existe una variacion de la Regresion Logistica para clasificar multiclases, donde el modelo es el siguiente:\n",
    "\n",
    "$$\n",
    "p(y=c|x, \\theta) = \\frac{exp(\\theta^T_c x)}{\\sum ^C_{v=1} exp(\\theta^T_v x)}\n",
    "$$\n",
    "\n",
    "donde C es el numero de clases. Esto se conoce comunmente como un **clasificador de maxima entropia**.\n",
    "En scikit learn, entrenar un regresor logistico binario o multiclase es transparente para el usuario.\n",
    "\n",
    "\n",
    "En las siguientes clases veremos como solucionar este problema utilizando tecnicas de agrupamiento de modelos. Pero antes, veremos 2 modelos no-parametricos utilizados en clasificacion multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4bfe7",
   "metadata": {},
   "source": [
    "## K-Nearest Neighboor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ca4fd",
   "metadata": {},
   "source": [
    "K Vecinos mas Cercanos es un modelo no parametrico comunmente usado para la clasificacion multiclase. Tambien puede ser usado para solucionar problemas de regresion.\n",
    "Dependiendo del problema, la salida del KNN sera:\n",
    "\n",
    "- En clasificacion es la clase, decidida a partir del voto de sus vecinos.\n",
    "- En regresion es un valor numero representando el promedio de la caracteristica correspondiente de los vecinos.\n",
    "\n",
    "Este modelo se caracteriza por estar dictado por los datos, donde los calculos se llevan acabo al momento de la prediccion. Ademas, al estar basado en distancias, las caracteristicas con distintas escalas (como unidades de medida) afectan fuertemente su rendimiento, requiriendo una normalizacion previa de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16022b",
   "metadata": {},
   "source": [
    "#### Funcionamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5566d",
   "metadata": {},
   "source": [
    "La fase de entrenamiento del modelo solo consiste en almacenar los datos. Esto se puede hacer bajo algun criterio como simplemente un vector o un arbol binario, para facilitar la busqueda luego.\n",
    "\n",
    "La fase de clasificacion consiste en tener un vector que quiero clasificar $x_i$ y encontrar los $K$ vecinos mas cercanos, es decir, que tengan la minima distancia total. Comunmente se utiliza la distancia euclideana para valores reales y la distancia hamming para discretos. Luego en base a la clase de los vecinos, asignarle una clase a $x_i$ bajo algun criterio, como mayoria. Tambien se puede ponderar la votacion de cada vecino con el inverso de su distancia, asi los vecinos mas lejanos importan menos en la decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea78b3",
   "metadata": {},
   "source": [
    "![](knn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacc87f",
   "metadata": {},
   "source": [
    "En Scikit Learn, KNN utiliza la misma logica del resto de modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9ef41",
   "metadata": {},
   "source": [
    "[sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "`KNeighborsClassifier.fit()` para ajustar el modelo (guardar los datos) \\\n",
    "`KNeighborsClassifier.predict()` para clasificar observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f2720",
   "metadata": {},
   "source": [
    "**Ventajas**\n",
    "\n",
    "- Rapido con pequenos y medianos datasets\n",
    "- Algoritmo simple, ademas el resultado es interpretable.\n",
    "- No se hacen supuestos de los datos, pues deja que los mismo decidan sobre nueva entrada. Util en escenarios no lineales.\n",
    "- Es simple y rapido agregarle mas informacion al modelo (una nueva observacion)\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "- La precision depende de la calidad de los datos. Esto significa que es sensible al ruido.\n",
    "- Con datasets grandes, el tiempo de prediccion puede ser alto.\n",
    "- Sensible a la escala de los datos y las caracteristicas irrelevantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bdf07",
   "metadata": {},
   "source": [
    "## Arbol de Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38be986",
   "metadata": {},
   "source": [
    "El Arbol de Decision (Decision Tree) es otro modelo no-parametrico capaz de hacer clasificacion y regresion. Se considera parte de los modelos con mejor interpretabilidad y sencillez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d284d",
   "metadata": {},
   "source": [
    "### ¿Qué se aprende y cómo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022d403",
   "metadata": {},
   "source": [
    "Para entender más precisamente cómo y qué <b>aprenden</b> los algoritmos de tipo Decision Tree, implantamos a continuación un algoritmo desde cero en Python. En camino, presentaremos los conceptos de <b>Gini impurity</b> e <b>Information Gain</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00429093",
   "metadata": {},
   "source": [
    "### 1. Algoritmos de tipo Decision Tree: CART\n",
    "\n",
    "Existen varias variaciones de algoritmos Decision Tree, entre las cuales: ID3, C4.5, C5.0, CART <a href=\"https://en.wikipedia.org/wiki/Decision_tree\">(ver wikipedia)</a>\n",
    "Cada algoritmo comparta la misma idea: a cada nivel del árbol de decisión, el modelo formula una pregunta permitiendo poco a poco de llegar a una predicción. ¿Cómo el algoritmo sabe qué preguntas hacer y en qué orden? Es lo que vamos a responder a través del ejemplo de CART. \n",
    "\n",
    "CART (Classification and Regression Trees) inicializa la raíz del árbol con el dataset de entranemiento completo (ver Figura). Luego busca dividir los datos del dataset con una pregunta. Los nodos siguientes reciben solamente los datos que corresponde a la respuesta. Se reitera este proceso hasta desenredar totalmente los datos (cada hoja del árbol debería tener idealmente un solo tipo de label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5de27",
   "metadata": {},
   "source": [
    "<img src=\"./img/CART.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46ede9",
   "metadata": {},
   "source": [
    "En nuestro ejemplo, el nodo 1 está totalmente desenredado (label: \"Grape\"). El nodo 2 tiene dos labeles, entonces preguntamos otra pregunta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7cea2",
   "metadata": {},
   "source": [
    "<img src=\"./img/CART2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c5d0b",
   "metadata": {},
   "source": [
    "Para construir un árbol eficiente, el punto importante es identificar qué preguntas formular y cuándo. Por lo tanto, necesitamos <b>cuantificar</b> en qué medida una pregunta permite desenredar los labeles. Para hacer eso se utiliza 2 métricas:\n",
    "- el coeficiente de '<b>Gini impurity</b>': mide que tan desenredados están los labeles de un nodo.\n",
    "- el coeficiente de '<b>Information Gain</b>': mide cuánto una pregunta permite bajar el 'Gini impurity'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9e6a1",
   "metadata": {},
   "source": [
    "<img src=\"./img/CART3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6b9ce",
   "metadata": {},
   "source": [
    "Utilizaremos estas métricas para estimar qué preguntas hacer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893460f2",
   "metadata": {},
   "source": [
    "### 2. ¿Qué preguntas formular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8412d",
   "metadata": {},
   "source": [
    "<img src=\"./img/CART4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c1689",
   "metadata": {},
   "source": [
    "Para saber qué preguntas formular, cada nodo itera sobre las características de los datos a su disposición y define una lista de preguntas posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f2327",
   "metadata": {},
   "source": [
    "### 3. Partición del dataset\n",
    "\n",
    "Una vez una pregunta elegida, se divide los datos en dos según la respuesta a la pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ada1b",
   "metadata": {},
   "source": [
    "<img src=\"./img/CART5.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8d0ac",
   "metadata": {},
   "source": [
    "### 4. Coeficiente de Gini impurity\n",
    "\n",
    "El coeficiente de Gini impurity representa la probabilidad de ser incorrecto si asigna aleatoriamente una etiqueta a un ejemplo del mismo conjunto. Este se calcula por nodo, como:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^J p(x)\\cdot p(1-x)\n",
    "$$\n",
    "\n",
    "Con J siendo la cantidad de elementos en la particion, y $p(x)$ la probabilidad de etiquetar incorrectamente un ejemplo nuevo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e288988",
   "metadata": {},
   "source": [
    "### 5. Information Gain\n",
    "\n",
    "La métrica de Information Gain permite medir qué pregunta optimiza el coeficiente de Gini impurity.\n",
    "\n",
    "Por cada nodo, empezamos por medir el coeficiente de Gini impurity de los labeles disponibles. Luego, por cada pregunta calculamos el coeficiente de Gini impurity de los dos sub-conjuntos de datos obtenidos.\n",
    "\n",
    "<img src=\"./img/CART-8.png\"></img>\n",
    "\n",
    "Calculamos la incerteza (<i>impurity</i>) promedia ponderada para los dos subconjuntos de datos obtenidos. Por ejemplo:\n",
    "\n",
    "<img src=\"./img/CART-9.png\"></img>\n",
    "\n",
    "Finalmente, conservamos la pregunta que permite optimizar la ganancia de información (Information Gain). En nuestro ejemplo:\n",
    "\n",
    "<b>Information Gain = 0.64 - 0.5 = 0.14</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4608c",
   "metadata": {},
   "source": [
    "Cuando se detiene el algoritmo del árbol de decisión?\n",
    "- Cuando todos los nodos son puros\n",
    "- Cuando se alcanza la profundidad máxima indicada por el usuario\n",
    "\n",
    "Respecto a la profundidad del árbol\n",
    "- Mientras más profundo es el árbol mayor es su complejidad\n",
    "- También se hace más propenso a sobre-ajustar el ruido en los datos\n",
    "\n",
    "Cómo evitar el sobreajuste? \\\n",
    "Poda (pruning): Eliminar ramas causadas por el ruido y peculiaridades \n",
    "del set de entrenamiento\n",
    "- Poda funciona de acuerdo al error de validación\n",
    "- Pre-poda: La rama no crece si incrementa el error de validación\n",
    "- Post-poda: La rama será removida si hacerlo disminuye el error de validación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0d53d",
   "metadata": {},
   "source": [
    "**Ventajas**\n",
    "- Sin parámetros sensibles\n",
    "- No hace supuesto sobre la distribución de los features\n",
    "- No necesita escalar las features (pre-processing)\n",
    "- Selección de características como parte del algoritmo\n",
    "- Maneja datos numéricos y categóricos\n",
    "- Robusto a outliers\n",
    "\n",
    "**Desventajas**\n",
    "- Fronteras de decisión perpendiculares a los ejes de características (hiper-rectángulo)\n",
    "- Inestable: Una pequeña cantidad de ruido produce un árbol muy distinto. \n",
    "- Tendencia al sobreajuste. Puede mitigarse usando poda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4ce28",
   "metadata": {},
   "source": [
    "En Scikit Learn usar un arbol sigue el mismo esquema que el resto de modelos:\n",
    "\n",
    "[sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "`DecisionTreeClassifier.fit()` para ajustar el modelo (guardar los datos) \\\n",
    "`DecisionTreeClassifier.predict()` para clasificar observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb3f9",
   "metadata": {},
   "source": [
    "Existen herramientas para exportar el arbol construido en una representacion grafica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b9377",
   "metadata": {},
   "source": [
    "Para esto es necesario instalar las librerias `pydot` y `graphviz`\n",
    "```python\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "\n",
    "clf = DcisionTreeClassifier()\n",
    "dot_data = StringIO()\n",
    "\n",
    "features=['Altura', 'Peso', 'Edad']\n",
    "classes=['0', '1']\n",
    "tree.export_graphviz(clf,out_file=dot_data,feature_names=features,class_names=classes, filled=True, \n",
    "                     rounded=True, impurity=False)\n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph[0].write_png('modelo.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b19c33",
   "metadata": {},
   "source": [
    "![](modelo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b286ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
